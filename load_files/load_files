#!/bin/bash

# Copyright 2019 ThoughtSpot
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation
# files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy,
# modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT
# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

# WARNING:  THIS FILE SHOULD NOT NEED TO BE MODIFIED.

# This file will load data files into ThoughtSpot using tsload.  It expects a configuration file to be provided.

# Needed when running from cron since this path is set in /home/admin/.bashrc - tsload is in this path.
PATH=$PATH:/usr/local/scaligent/bin:/usr/local/scaligent/release/bin
declare -r EXIT_OK=0
declare -r EXIT_ERROR=1

#--[function usage()]-----------------------------------------------------------------------------------------
#
#  Shows the usage instructions of this script
#-------------------------------------------------------------------------------------------------------------
function usage() {
  exit_code=$1

  echo ""
  echo "usage: ${0} -f CONFIG_FILE"
  echo "  where CONFIG_FILE is a configuration file for the loading."
  echo ""
  exit ${exit_code}
}

#--[function log_this()]---------------------------------------------------------------------------------------
#
# Writes a message to the logfile:
# param 1 - The message to log
# param 2 - The priority of the message (currently free text, TODO: implement logging levels similar as verbose levels
# param 3 - (OPTIONAL) name of the log file, if not specified the summary log file will be used
#-------------------------------------------------------------------------------------------------------------
function log_this() {
  log_datetime=$(date --rfc-3339=seconds)
  log_message="${1}"
  log_priority="${2}"
  log_file=${3:-${TEMP_RESULTS_SUMMARY_FILE}}

  printf "%s | PID %s | [%-10s] | %s\n" "${log_datetime}" "$$" "${log_priority}" "${log_message}" >> "${log_file}"
}

#--[function check_directories()]-----------------------------------------------------------------------------
#
# Makes sure correct directories exist and creates where appropriate or exits with an error.
#-------------------------------------------------------------------------------------------------------------
function check_directories() {

  if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then
    echo ""
    # don't do anything currently.  Folders will be created automatically as needed.
  else # default to file system.
    if [[ ! -e ${DATA_DIR} ]]; then
      log_this "Required data directory ${DATA_DIR} not found.  Exiting." "ERROR"
      exit ${EXIT_ERROR}
    fi
    if [[ ! -e ${OLD_DIR} ]]; then mkdir -p "${OLD_DIR}"; fi
    if [[ ! -e ${OLD_DIR}/data ]]; then mkdir -p "${OLD_DIR}/data"; fi
  fi

  # Common directories for all sources.
  if [[ ! -e ${LOG_DIR} ]]; then mkdir -p "${LOG_DIR}"; fi
}

#--[function check_for_semaphore()]---------------------------------------------------------------------------
#
# See if there is a semaphore file.  If so, see if it exists.
#-------------------------------------------------------------------------------------------------------------
check_for_semaphore() {

  # AWS S3 requires a different check than the file system.
  if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then

    if [[ ${SEMAPHORE_FILE_NAME} != "" ]]; then
      aws s3 ls s3://${AWS_S3_BUCKET}/${SEMAPHORE_FILE_NAME} 
      if [[ $? != 0 ]]; then  # semaphore doesn't exist.
        echo "no semaphore file s3://${AWS_S3_BUCKET}/${SEMAPHORE_FILE_NAME} exists."
        exit ${EXIT_OK}
      fi
    fi

  else # default to file system.

    if [[ ${SEMAPHORE_FILE_NAME} != "" ]]; then
      if [[ ! -f ${DATA_DIR}/${SEMAPHORE_FILE_NAME} ]]; then
        echo "no semaphore file ${DATA_DIR}/${SEMAPHORE_FILE_NAME} exists."
        exit ${EXIT_OK}
      fi
    fi

  fi
}

#--[function check_already_loading()--------------------------------------------------------------------------
#
# Make sure there is only one load process at a time.  Also recover if a process failed.
# NOTE that this checks the local process space, so it's possible another load is running on another machine.
# TODO:  Create a process for handling multi-node environments to prevent loads on another machine.
#-------------------------------------------------------------------------------------------------------------
function check_already_loading() {
  if [[ -f ${LOADING_FILE} ]]; then
    other_pid=`head -n 1 ${LOADING_FILE}`
    running_pid=`ps -ef | awk '{ print $2 }' | grep $other_pid`
    if [[ "$running_pid" == "" ]]; then
      echo $$ > "${LOADING_FILE}"
      log_this "Taking over from stopped process $other_pid" "INFO"
    else
      exit ${EXIT_OK}
    fi
  else
    echo $$ > "${LOADING_FILE}"
  fi
}

#--[function contains()]--------------------------------------------------------------------------------------
#
# Function to see if an element exists in an array
#-------------------------------------------------------------------------------------------------------------
function contains() {
  local n=$#
  local value=${!n}
  for ((i=1;i < $#;i++)) {
    if [[ "${!i}" == "${value}" ]]; then
      return 0
    fi
  }
  return 1
}

# Variables to log errors and loading.
has_tsload_errors=false
number_successful_loads=0
total_attempted_files=0

# This variable contains tables that have been truncated before loading parts.
# This only supports one database.
truncated_tables=()
# This variable contains tables for which an sql statement has been run before loading
# run_sql_tables=()
pre_tql_tables=()
post_tql_tables=()
pre_shell_tables=()
post_shell_tables=()


#--[function wait_to_complete ()]-----------------------------------------------------------------------------
#
# Waits until the process with pid has completed with a maximum of SCRIPT_MAX_ATTEMPT
#-------------------------------------------------------------------------------------------------------------
function wait_to_complete() {
  pid=$1

  log_this "Waiting for process ${pid} to complete" "INFO"

  START=1
  END=${SCRIPT_MAX_CHECK_ATTEMPTS}

  i=$START
  while [[ $i -le $END ]]
  do
      output=$(ps -p "${pid}")
      if [ "$?" -eq 0 ]; then
        log_this "Process ${pid} not completed yet. Check ${i}" "DEBUG"
        sleep "${SCRIPT_CHECK_WAIT_TIME}"
      else
        log_this "Process ${pid} completed after ${i} checks" "INFO"
        break
      fi
      ((i = i + 1))
  done
}

#--[function run_sql ()]--------------------------------------------------------------------------------------
#
# Runs a sql query against the ThoughtSpot database
#-------------------------------------------------------------------------------------------------------------
function run_sql() {
  sql=$1

  log_this "Running query: ${sql}" "INFO"
  tmp_file="/tmp/$$.tmp_query.out"

  echo ${sql} | tql 2>${tmp_file} & export TQL_PID=$!  
  log_this "SQL pid is ${TQL_PID}" "INFO"

  wait_to_complete ${TQL_PID}

  if grep --quiet "Statement executed successfully." ${tmp_file}; then
    log_this "Query ran successfully" "INFO"
    return 0
  else
    errmsg=`cat ${tmp_file}`
    log_this "Query failed to executed. Message returned: ${errmsg}" "ERROR"
    return 1
  fi
  # Cleanup temp file
  rm -f ${tmp_file}
}

#--[function parse_tsload_results()]--------------------------------------------------------------------------
#
# Parses and summarises the output of the tsload process and writes it to the temporary stats file
#-------------------------------------------------------------------------------------------------------------
function parse_tsload_results () {
  schema_name=$1
  table_name=$2
  file_name=$3

  status=`cat ${TEMP_TSLOAD_FILE} | grep "^Status" | sed 's/^.*: //' |sed 's/^[ \t]*//' `   
  rows_total=`cat ${TEMP_TSLOAD_FILE} | grep "^Rows total" | sed 's/^.*: //' | sed 's/[[:space:]]\+//g'`
  rows_loaded=`cat ${TEMP_TSLOAD_FILE} | grep "^Rows successfully loaded" | sed 's/^.*: //' | sed 's/[[:space:]]\+//g'`
  rows_failed=`cat ${TEMP_TSLOAD_FILE} | grep "^Rows failed to load" | sed 's/^.*: //' | sed 's/[[:space:]]\+//g'`
  rows_skipped=`cat ${TEMP_TSLOAD_FILE} | grep "^Rows duplicate/omitted" | sed 's/^.*: //' | sed 's/[[:space:]]\+//g'`
  rows_loaded_perc=`cat ${TEMP_TSLOAD_FILE} | grep "^% of Rows successfully loaded" | sed 's/^.*: //' | sed 's/[[:space:]]\+//g'`
  echo "${DATABASE_NAME}|${schema_name}|${table_name}|${status}|${rows_total}|${rows_loaded}|${rows_failed}|${rows_skipped}|$rows_loaded_perc|${file_name}" >> ${TEMP_STATS_FILE}

  cat ${TEMP_TSLOAD_FILE} >> ${TEMP_RESULTS_FILE}
}

#--[function run_action()]-----------------------------------------------------------------------------------
#
# Run an action, either a TQL command or a Shell Script
#-------------------------------------------------------------------------------------------------------------
function run_action() {
  schema_name=$1
  table_name=$2
  config_file=$3
  file_name=$4
  action_type=$5
  action_when=$6
  action_code=$7
  shift 7
  processed_tables=("$@")

  result=1
  contains ${processed_tables[@]} "${schema_name}"."${table_name}"
  if [[ $? != 0 ]]; then 
    result=0

    # Need to handle spaces in names and archive_queries.  This assumes no colons in the query.
    OLD_IFS=$IFS
    IFS=$(echo -en ":\n")

    if [[ "${action_code}" != "" ]]; then
      log_this "Running ${action_type} ${action_when} the loading of ${DATABASE_NAME}.${schema_name}.${table_name} from file ${file_name}. The command is: ${script}" "INFO"

      if [[ "${action_type}" == "shell script" ]]; then
        log_this "Executing shell script" "INFO"
        script="${action_code} -d \"${DATABASE_NAME}\" -s \"${schema_name}\" -t \"${table_name}\" -c \"${config_file}\" -l \"${TEMP_RESULTS_SUMMARY_FILE}\" -f \"${file_name}\""
        # Run command in a sub shell
        (eval "${script}") & export SHELL_PID=$!
        log_this "${action_type} pid is ${SHELL_PID}" "INFO"
        wait_to_complete ${SHELL_PID}
      elif [[ "${action_type}" == "TQL" ]]; then
        log_this "Executing TQL" "INFO"
        script="${action_code}"
        if run_sql "${script}"; then
          log_this "Query completed successfully." "SUCCESS"
        else
          log_this "Query failed." "ERROR"
        fi
      fi
    fi

    # Reset the separator to the original values.
    IFS=$OLD_IFS
  fi
  return ${result}
}

#--[function load_a_file()]-----------------------------------------------------------------------------------
#
# Loads one file into a table using tsload.
#-------------------------------------------------------------------------------------------------------------
function load_a_file() {
  orig_file_name=$1
  
  # Determine schema name and table name
  if [[ $orig_file_name == *"/"* ]]; then
    schema_name=$(echo $orig_file_name | cut -d"/" -f 1)
    file_name=$(echo $orig_file_name | cut -d"/" -f 2)
  else
    schema_name=${DEFAULT_SCHEMA_NAME}
    file_name=$orig_file_name
  fi

  # the extension and anything after a - will be automatically removed.  Neither can be used in the table name.
  # see if the file name overrides the default empty target.
  # use all of the patterns to get the table name from the file name.
  if [[ ${file_name} == *"_full"* ]]; then
    empty_target="--empty_target"
  elif [[ ${file_name} == *"_incremental"* ]]; then
    empty_target=""
  else
    empty_target=${DEFAULT_EMPTY_TARGET}
  fi

  table_name="`echo ${file_name} | sed s/${DATA_FILE_EXTENSION}// | sed s/-.*// | sed s/_full// | sed s/_incremental//`"
  for pattern in ${SED_PATTERNS[*]}; do
    table_name="`echo ${table_name} | sed s/${pattern}//`"
  done

  log_this "Loading file ${file_name} into database ${DATABASE_NAME}, schema ${schema_name} and table ${table_name}" "INFO"

  # Directory for loaded data and bad records.
  move_dir="${OLD_DIR}/data/${schema_name}"

  if [[ ${SOURCE_TYPE} == "file_system" ]]; then
    # Create move directory if it does not exist
    if [[ ! -e ${move_dir} ]]; then mkdir -p ${move_dir}; fi
  fi

  total_attempted_files=$((total_attempted_files + 1))

  # See if we should truncate the table before loading.
  if [[ ${TRUNCATE_BEFORE_LOAD} == "true" ]]; then
    contains ${truncated_tables[@]} "${schema_name}"."${table_name}"
    if [[ $? != 0 ]]; then  # this is table that should be truncated.
      truncated_tables+=( "${schema_name}"."${table_name}" )
      sql="truncate table ${DATABASE_NAME}.${schema_name}.${table_name};"
      if run_sql "${sql}"; then
        log_this "Table ${DATABASE_NAME}.${schema_name}.${table_name} truncated successfully." "SUCCESS"
      else
        log_this "Table ${DATABASE_NAME}.${schema_name}.${table_name} could not be truncated." "ERROR"
      fi
    fi
  fi

  # --------------------------------------------------------------------------------------------------
  # Pre shell scripts
  # --------------------------------------------------------------------------------------------------
  if run_action "${schema_name}" \
                "${table_name}" \
                "${config_file}" \
                "${file_name}" \
                "shell script" \
                "before" \
                "${pre_load_shell[$table_name]}" \
                "${pre_shell_tables[@]}" ; then
      pre_shell_tables+=("${schema_name}"."${table_name}")
  fi


  # --------------------------------------------------------------------------------------------------
  # Pre TQL scripts
  # --------------------------------------------------------------------------------------------------
  if run_action "${schema_name}" \
                "${table_name}" \
                "${config_file}" \
                "${file_name}" \
                "TQL" \
                "before" \
                "${pre_load_tql[$table_name]}" \
                "${pre_tql_tables[@]}" ; then
      pre_tql_tables+=("${schema_name}"."${table_name}")
  fi

  # --------------------------------------------------------------------------------------------------
  # Do the actual load process
  # --------------------------------------------------------------------------------------------------
  # get the header flag.
  hhr=""
  if [[ ${HAS_HEADER_ROW} == "true" ]]; then
    hhr="--has_header_row"
  fi


  # Write the file name to the TEMP RESULTS FILE, as the detail section does not have any reference to it, so it will make debugging easier
  # i.e. for while file the details are shown
  log_this "Detailed loading results for input file: ${file_name}" "INFO" ${TEMP_RESULTS_FILE}


  # Define the settings required for the tsload depending on whether this is local or AWS loading
  if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then
    log_this "Running tsload for AWS S3 load" "INFO"
    source_file="/aws/default/${orig_file_name}"

    # If configured, run a dos2unix on the source file before loading
    # if [[ ${RUN_DOS2UNIX} == 1 ]]; then
    #   dos2unix "${source_file}"
    # fi

    # If there are no access keys, then don't set the credentials.
    if [[ ${AWS_S3_ACCESS_KEY_ID} == "" ]]; then  # not using access keys
      AWS_CREDENTIALS=""
    else
      AWS_CREDENTIALS="--aws_s3_credentials \"${AWS_S3_ACCESS_KEY_ID};${AWS_S3_SECRET_ACCESS_KEY}\""
    fi
    
    # The specific flags are set in the configuration file, q.v..
    tsload --source_file ${source_file} \
      --target_database ${DATABASE_NAME} \
      --target_schema ${schema_name} \
      --target_table "${table_name}" \
      ${empty_target} \
      ${hhr} \
      --source_data_format ${SOURCE_DATA_FORMAT} \
      --field_separator "${FIELD_SEPARATOR}" \
      --max_ignored_rows ${MAX_IGNORED_ROWS} \
      --null_value "${NULL_VALUE}" \
      --date_format "${DATE_FORMAT}" \
      --date_time_format "${DATE_TIME_FORMAT}" \
      --time_format "${TIME_FORMAT}" \
      --boolean_representation ${BOOLEAN_REPRESENTATION} \
      --skip_second_fraction \
      -v ${V_LEVEL} \
      --aws_s3_bucket_name "${AWS_S3_BUCKET}" \
      ${AWS_CREDENTIALS} \
      --aws_s3_region "${AWS_S3_REGION}" \
      --aws_s3_root "${AWS_S3_DATA_DIR}" \
      --buf_capacity ${AWS_S3_BUF_CAPACITY} \
       > ${TEMP_TSLOAD_FILE} 2>&1

  else
    log_this "Running tsload for file system load" "INFO"
    source_file="${DATA_DIR}/${orig_file_name}"
    log_this "Source file=${source_file}" "INFO"

    # If configured, run a dos2unix on the source file before loading
    if [[ ${RUN_DOS2UNIX} == 1 ]]; then
      dos2unix "${source_file}"
    fi

    # The specific flags are set in the configuration file, q.v..
    header=""
    if [[ ${#extra_table_headers[@]} > 0 ]]; then
      if [[ ${extra_table_headers[$table_name]} ]]; then
        header=${extra_table_headers[$table_name]}
      fi
    fi
    values=""
    if [[ ${#extra_table_values[@]} > 0 ]]; then
      if [[ ${extra_table_values[$table_name]} ]]; then
        values=${extra_table_values[$table_name]}
      fi
    fi
    cat ${source_file} | 
      awk -v h="${header}" -v val="${values}" -v sep="${FIELD_SEPARATOR}" '{ if(h!="") { if(NR==1){ print $0 sep h } else { print $0 sep val } } else { print $0 } }' | 
      tsload \
      --target_database ${DATABASE_NAME} \
      --target_schema ${schema_name} \
      --target_table "${table_name}" \
      --bad_records_file ${move_dir}/${table_name}_bad_records.csv \
      ${empty_target} \
      ${hhr} \
      --source_data_format ${SOURCE_DATA_FORMAT} \
      --field_separator "${FIELD_SEPARATOR}" \
      --max_ignored_rows ${MAX_IGNORED_ROWS} \
      --null_value "${NULL_VALUE}" \
      --date_format "${DATE_FORMAT}" \
      --date_time_format "${DATE_TIME_FORMAT}" \
      --time_format "${TIME_FORMAT}" \
      --boolean_representation ${BOOLEAN_REPRESENTATION} \
      --skip_second_fraction \
      -v ${V_LEVEL} \
       > ${TEMP_TSLOAD_FILE} 2>&1
  fi
 
  if [[ $? != 0 ]]; then
    has_tsload_error=true
    log_this "tsload failed to load ${file_name} into ${DATABASE_NAME}.${schema_name}.${table_name}" "ERROR"
    number_failed_loads=$((number_failed_loads+1))
  else
    log_this "${file_name} loaded successfully into ${DATABASE_NAME}.${schema_name}.${table_name}" "SUCCESS"
    number_successful_loads=$((number_successful_loads+1))
  fi

  parse_tsload_results ${schema_name} ${table_name} "${file_name}"
  

  # --------------------------------------------------------------------------------------------------
  # Post TQL scripts
  # --------------------------------------------------------------------------------------------------
  if run_action "${schema_name}" \
                "${table_name}" \
                "${config_file}" \
                "${file_name}" \
                "TQL" \
                "after" \
                "${post_load_tql[$table_name]}" \
                "${post_tql_tables[@]}" ; then
      post_tql_tables+=("${schema_name}"."${table_name}")
  fi

  # --------------------------------------------------------------------------------------------------
  # Post shell scripts
  # --------------------------------------------------------------------------------------------------
  if run_action "${schema_name}" \
                "${table_name}" \
                "${config_file}" \
                "${file_name}" \
                "shell script" \
                "after" \
                "${post_load_shell[$table_name]}" \
                "${post_shell_tables[@]}" ; then
      post_shell_tables+=("${schema_name}"."${table_name}")
  fi

  # Move the loaded files to the old directory.
  if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then
    # Currently only mv and cp is supported for S3.  Others will probably fail.
    echo "copying from bucket ${AWS_S3_BUCKET}"
    if [[ ${schema_name} != ${DEFAULT_SCHEMA_NAME} ]]; then
      aws s3 ${MOVE_LOADED_FILES} s3://${AWS_S3_BUCKET}/${schema_name}/${file_name} ${OLD_DIR}/${schema_name}/${file_name}
    else
      aws s3 ${MOVE_LOADED_FILES} s3://${AWS_S3_BUCKET}/${file_name} ${OLD_DIR}/${schema_name}/${file_name}
    fi  
  else
    ${MOVE_LOADED_FILES} "${source_file}" "${move_dir}"
  fi
}

#--[function should_ignore()]---------------------------------------------------------------------------------
#
#  Checks whether this directory should be ignored
#-------------------------------------------------------------------------------------------------------------
function should_ignore() {
  local fn=$1
  OLD_IFS=${IFS}
  IFS="/" read -r dir_to_check string <<< "${fn}"
  IFS=${OLD_IFS}

  for ignore in ${IGNORE_DIRS[*]}; do [[ "${ignore}" == "${dir_to_check}" ]] && return 0; done
  return 1
}

function load_data_files() {
  exclude=""
  if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then
    source_location=${AWS_S3_BUCKET}
    if [[ ${EXCLUDE_PATTERN} != "" ]]; then
      exclude="| grep -v *${EXCLUDE_PATTERN}"
      log_this "Excluding files of pattern *${EXCLUDE_PATTERN}" "INFO"
    fi
    files_to_load=`aws s3 ls --recursive s3://${AWS_S3_BUCKET} | awk '{$1=$2=$3=""; print $0}' | sed 's/^[ \t]*//' | grep "${DATA_FILE_EXTENSION}$"`
  else
    source_location=${DATA_DIR}
    if [[ ${EXCLUDE_PATTERN} != "" ]]; then
      exclude="! -iname *${EXCLUDE_PATTERN} " 
      log_this "Excluding files of pattern *${EXCLUDE_PATTERN}" "INFO"
    fi
    files_to_load=`find ${DATA_DIR} -maxdepth 2 -iname "*${DATA_FILE_EXTENSION}" ${exclude} -print`
    log_this "Executing: find ${DATA_DIR} -maxdepth 2 -iname *${DATA_FILE_EXTENSION} ${exclude} -print "
  fi
  
  if [[ $files_to_load != "" ]]; then
    # load all data files, one at a time.
    OLD_IFS=$IFS
    IFS=$'\n'
    for fn in ${files_to_load}; do
      local schema_name=${DEFAULT_SCHEMA_NAME}
      
      if [[ ${SOURCE_TYPE} == "file_system" ]]; then
        # for local files remove the base data dir
        fn=${fn#"$DATA_DIR"}
        # and potentially the last forward slash
        fn=${fn#"/"}
      fi
       
      should_ignore ${fn}  # makes sure not an ignore directory.
      if [[ $? != 0 ]]; then
        load_a_file "${fn}"
      fi

    done
    IFS=$OLD_IFS
  else
    log_this "No ${DATA_FILE_EXTENSION} files found in ${source_location}" "WARNING"
  fi
}

#--[function cleanup_from_local_load()]-----------------------------------------------------------------------------
#
# Clean up files, archiving data, etc.
#-------------------------------------------------------------------------------------------------------------
function cleanup_from_local_load() {

  log_this "Cleaning up from load" "INFO"
  log_this "Archiving option: ${ARCHIVE_DATA}" "INFO"
  log_this "Move loaded files option: ${MOVE_LOADED_FILES}" "INFO"
  log_this "Purging files older than ${NBR_DAYS_TO_KEEP_OLD_FILES} days from ${OLD_DIR_ROOT} and ${LOG_DIR}" "INFO"

  # Check if bad records where generated -- only for local file system loads.
  has_bad_records=false
  for f in ${OLD_DIR}/data/${DEFAULT_SCHEMA_NAME}/*_bad_records.csv; do
    [ -e "${f}" ] && has_bad_records=true || has_bad_records=false
    break
  done

  # Move the loaded files to the old directory.
  # if there were files loaded, save the results
  if [[ ${total_attempted_files} != 0 ]]; then
    log_this "Loading Summary:" "INFO" ${RESULTS_FILE}
    echo -en '\n' >> ${RESULTS_FILE}
    cat ${TEMP_STATS_FILE} | column -s '|' -t -o '|' >> ${RESULTS_FILE}
    echo -en '\n' >> ${RESULTS_FILE}

    cat ${TEMP_RESULTS_SUMMARY_FILE} >> ${RESULTS_FILE}
    cat ${TEMP_RESULTS_FILE} >> ${RESULTS_FILE}

    if [[ "${ARCHIVE_DATA}" == "always" || ( "${ARCHIVE_DATA}" == "onerror" && ${has_tsload_error} = true ) || ( "${ARCHIVE_DATA}" == "onbad" && ${has_bad_records} = true ) ]] ; then
      cp ${RESULTS_FILE} ${OLD_DIR}
      pushd . 2&>/dev/null
      cd ${OLD_DIR}/.. && tar czf ${OLD_DIR}.tar.gz ${THE_DATE_TIME} && rm -r ${OLD_DIR}
      popd 2&>/dev/null
    else
      rm -r ${OLD_DIR}
    fi

    # clear out the old archives to save space if the value is defined.
    if [[ ${NBR_DAYS_TO_KEEP_OLD_FILES} ]]; then

      find ${OLD_DIR_ROOT} -type f -mtime +${NBR_DAYS_TO_KEEP_OLD_FILES} -name '*.gz' -execdir rm -- '{}' \;
      find ${LOG_DIR} -type f -mtime +${NBR_DAYS_TO_KEEP_OLD_FILES} -name '*.txt' -execdir rm -- '{}' \;
    fi
  else
    rm ${TEMP_RESULTS_SUMMARY_FILE}
    rm -r ${OLD_DIR}
  fi

  rm ${LOADING_FILE} # remove the loading semaphore file

  if [[ -f ${DATA_DIR}/${SEMAPHORE_FILE_NAME} ]]; then
    rm ${DATA_DIR}/${SEMAPHORE_FILE_NAME}
  fi
}

#--[function cleanup_from_s3_load()]-----------------------------------------------------------------------------
#
# Clean up files, archiving data, etc.
#-------------------------------------------------------------------------------------------------------------
function cleanup_from_s3_load() {

  log_this "Cleaning up from load" "INFO"
  log_this "Archiving option: ${ARCHIVE_DATA}" "INFO"
  log_this "Move loaded files option: ${MOVE_LOADED_FILES}" "INFO"

  # Move the loaded files to the old directory.
  # if there were files loaded, save the results
  if [[ ${total_attempted_files} != 0 ]]; then
    log_this "Loading Summary:" "INFO" ${RESULTS_FILE}
    echo -en '\n' >> ${RESULTS_FILE}
    cat ${TEMP_STATS_FILE} | column -s '|' -t -o '|' >> ${RESULTS_FILE}
    echo -en '\n' >> ${RESULTS_FILE}

    cat ${TEMP_RESULTS_SUMMARY_FILE} >> ${RESULTS_FILE}
    cat ${TEMP_RESULTS_FILE} >> ${RESULTS_FILE}

    results_file_name=`echo "${RESULTS_FILE}" | sed 's/.*\///'`
    aws s3 cp ${RESULTS_FILE} ${OLD_DIR}/${results_file_name}
  fi

  rm ${LOADING_FILE} # remove the loading semaphore file

  # if the semaphore exists, remove it.  Need to only do if it exists since otherwise can cause issues.
  aws s3 ls s3://${AWS_S3_BUCKET}/${SEMAPHORE_FILE_NAME} 
  if [[ $? == 0 ]]; then  # semaphore exists.
    aws s3 rm s3://${AWS_S3_BUCKET}/${SEMAPHORE_FILE_NAME} 
  fi
}

#--[function send_results_notification()]---------------------------------------------------------------------
#
# Sends email to indicate the results of the load.
#-------------------------------------------------------------------------------------------------------------
function send_results_notification() {

  # only send if there were files that attempted to load.
  if [[ ${total_attempted_files} != 0 ]]; then
    results_table=`cat ${TEMP_STATS_FILE} | column -s '|' -t -o '|'`
    subject="Success:  ${number_successful_loads} of ${total_attempted_files} files loaded at ${THE_DATE_TIME} for cluster ${CLUSTER_NAME}"
    if [[ ${has_tsload_error} = true ]]; then
      subject="Error:  ${number_successful_loads} of ${total_attempted_files} files loaded and ${number_failed_loads} failed at ${THE_DATE_TIME} for cluster ${CLUSTER_NAME}"
      body="The data load ${THE_DATE_TIME} for cluster ${CLUSTER_NAME} had errors loading files or rows.  See attached load results."
      echo ${body}
      if [[ ${USE_HTML_EMAIL} == 1 ]]; then
        body="The data load <b>${THE_DATE_TIME}</b> for cluster <b>${CLUSTER_NAME}</b> had <b>errors</b> loading files or rows.  See attached load results."
      fi
      exit_value=1
    else
      subject="Success:  ${number_successful_loads} of ${total_attempted_files} files loaded at ${THE_DATE_TIME} for cluster ${CLUSTER_NAME}"
      body="The data load ${THE_DATE_TIME} for cluster ${CLUSTER_NAME} appears successful.  See attached load results."
      echo ${body}
      if [[ ${USE_HTML_EMAIL} == 1 ]]; then
        body="The data load <b>${THE_DATE_TIME}</b> for cluster <b>${CLUSTER_NAME}</b> appears <b>successful</b>.  See attached load results."
      fi
    fi

    cat ${TEMP_STATS_FILE} | column -s '|' -t -o '|' > /tmp/$$.load_summary.txt
    for address in ${RESULTS_EMAIL[*]}; do
      if [[ ${USE_HTML_EMAIL} == 0 ]]; then
        # TODO remove after working.
        echo ""
        echo "${body}" | mail -s "${subject}" -a ${RESULTS_FILE} -a /tmp/$$.load_summary.txt ${address}
      else
        from="admin@thoughtspot.com"

        # AWK script to generate HTML table
        result_table=` \
          awk '# Set field separator as pipe and write out the table start tag 
               BEGIN {
                 FS="|";
                 print "<table>"
               }
          # Function to print a row with one argument to handle either a th tag or td tag
          # This function expects the 4th parameter to be the tsload status (Successful or something else)
          # The class name will be set to success or failed based on the value of this parameter 
          # (Allowing css styling for the table)
          function printRow(tag) {
            if ( $4 == "Successful" ) {
              print "<tr class=\"success\">";
            } else {
              print "<tr class=\"failed\">";
            }
      
            for(i=1; i<=NF; i++) print "<"tag">"$i"</"tag">";
              print "</tr>"
          }
        
          # If row number (NR variable) is 1, call printRow fucntion with 'th' as argument
          NR==1 {
            printRow("th")
          }
          # If row number (NR variable) is greater than 1, call printRow fucntion with 'td' as argument
          NR>1 {
            printRow("td")
          }
          # Print table footer
        
          END {
            print "</table>"
          }' ${TEMP_STATS_FILE}`

        # Format HTML email
        (
          echo "From: ${from}"
          echo "To: ${address}"
          echo "Subject: ${subject}"
          echo "Mime-Version: 1.0"
          echo 'Content-Type: multipart/mixed; boundary="GvXjxJ+pjyke8COw"'
          echo "Content-Disposition: inline"
          echo ""
          echo "--GvXjxJ+pjyke8COw"
          echo "Content-Type: text/html"
          echo "Content-Disposition: inline"
          echo "<style>"
          echo "  table,th,td { border: 1px solid black; border-collapse:collapse; font-size:12px;}"
          echo "  th          { text-align: left; background-color: black; color: white; font-weight: normal; padding: 5px;} "
          echo "  td          { padding: 5px; } "
          echo "  tr.success  { background-color: #99c140; } "
          echo "  tr.failed   { background-color: #cc3232; color:white; font-weight:bold } "
          echo "</style>"
          echo "<p>${body}</p>"
          echo "${result_table}"
          echo ""
          echo "--GvXjxJ+pjyke8COw"
          echo "Content-Type: text/plain"
          echo "Content-Disposition: attachement; filename=loading_log_file.txt"
          echo ""
          cat ${RESULTS_FILE}
        ) | /usr/lib/sendmail -t
      fi
    done
  fi

}


#-------------------------------------------------------------------------------------------------------------
#---------------------------------------- Main execution of script -------------------------------------------
#-------------------------------------------------------------------------------------------------------------

# Get the input parameters and verify there is a file to use for configs.
while getopts 'f:h' opt
do
  case ${opt} in
    f) config_file=${OPTARG}
    ;;
    h|?) usage 0
    ;;
  esac
done

if [[ ! -f "${config_file}" ]]; then
  usage -1
fi

# Read the configuration file
source ${config_file}

if [[ ${SOURCE_TYPE} == "aws_s3" && "${AWS_S3_ACCESS_KEY_ID}" != ""  ]]; then
  export AWS_ACCESS_KEY_ID=${AWS_S3_ACCESS_KEY_ID}
  export AWS_SECRET_ACCESS_KEY=${AWS_S3_SECRET_ACCESS_KEY}
fi

# Check if a sempahore file is used, and if so, if the process should run, may exit
check_for_semaphore

# Check for the loading semaphore, will exit if other process is running
check_already_loading

# Check if all required folders exist and create if not, may exit with error
check_directories

# Initialise the log summary file
touch ${TEMP_RESULTS_SUMMARY_FILE}

# Write the header to the stats summary file
echo "Database Name|Schema Name|Table Name|Status|Rows Total|Rows Loaded|Rows Failed|Rows Skipped|Rows Loaded %|File Name" > ${TEMP_STATS_FILE}

# --------------------------------------------------------------------------------------------------
# Pre load shell script
# --------------------------------------------------------------------------------------------------
log_this "Running start of process Shell"
table_name="@LOAD"
# Deliberately passing in this tablename in other parameters as they are not required for the one-off actions
run_action  "${table_name}" \
            "${table_name}" \
            "${config_file}" \
            "${table_name}" \
            "shell script" \
            "before" \
            "${pre_load_shell[$table_name]}" \
            "" 

# --------------------------------------------------------------------------------------------------
# Pre load TQL script
# --------------------------------------------------------------------------------------------------
log_this "Running start of process TQL"
table_name="@LOAD"
# Deliberately passing in this tablename in other parameters as they are not required for the one-off actions
run_action  "${table_name}" \
            "${table_name}" \
            "${config_file}" \
            "${table_name}" \
            "TQL" \
            "before" \
            "${pre_load_tql[$table_name]}" \
            "" 

load_data_files

# --------------------------------------------------------------------------------------------------
# Post load TQL script
# --------------------------------------------------------------------------------------------------
log_this "Running end of process TQL"
table_name="@LOAD"
# Deliberately passing in this tablename in other parameters as they are not required for the one-off actions
run_action  "${table_name}" \
            "${table_name}" \
            "${config_file}" \
            "${table_name}" \
            "TQL" \
            "after" \
            "${post_load_tql[$table_name]}" \
            "" 

# --------------------------------------------------------------------------------------------------
# Post load shell script
# --------------------------------------------------------------------------------------------------
log_this "Running end of process Shell"
table_name="@LOAD"
# Deliberately passing in this tablename in other parameters as they are not required for the one-off actions
run_action  "${table_name}" \
            "${table_name}" \
            "${config_file}" \
            "${table_name}" \
            "shell script" \
            "after" \
            "${post_load_shell[$table_name]}" \
            "" 

# Clean up the data loads (different for AWS S3 and local)
if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then
  cleanup_from_s3_load
else 
  cleanup_from_local_load
fi

# Sends a notification of the results
send_results_notification

# If any file has been loaded, list the contents of the log file to stdout
if [[ ${total_attempted_files} != 0 ]]; then
  cat ${RESULTS_FILE}
else
  log_this "No files loaded." "INFO"
fi

